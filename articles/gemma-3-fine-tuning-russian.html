<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" href="../styles/main.css">
    <script src="../scripts/gemma-3-fine-tuning.js" defer></script>

    <title>AI Under Hood</title>
</head>
<body>
    <div class="content">
        <header class="header-container">
            <span>
                <a href="../index-russian.html">–ì–ª–∞–≤–Ω–∞—è</a> ¬©&nbsp;Adil Arshidin
            </span>
            <span class="header-container__nav">
                <a href="#">LLM –ë–µ–Ω—á–º–∞—Ä–∫</a>&nbsp;¬∑
                <a href="../knowledge_base-russian.html">–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π</a>
            </span>
            <span>
                <a href="#">DE üá©üá™</a>&nbsp;/
                <a href="gemma-3-fine-tuning-russian.html">RU üá∑üá∫</a>&nbsp;/
                <a href="gemma-3-fine-tuning.html">EN üá¨üáß</a>
            </span>
        </header>
        <main class="main-container">
            <article>
                <h3>Fine-Tuning Gemma 3 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.</h3>
                <p>–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ <a href="https://ai.google.dev/gemma/docs/core/huggingface_vision_finetune_qlora" target="_blank">–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–µ</a>, –Ω–æ –≤ –Ω–µ–π —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ, –∏ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.‚Äã</p>
                <p>–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ: –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤ Linux –Ω–∞ –±–∞–∑–µ Ubuntu</p>
                <br>

                <h4>–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è</h4>
                <p>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Python 3.11.7. –≠—Ç—É –≤–µ—Ä—Å–∏—é –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è <a href="https://github.com/pyenv/pyenv" target="_blank">pyenv</a>. –ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ pyenv –∏ –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–µ–∫—Ç–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: <code>pyenv local 3.11.7</code>.</p>
                <p>–ó–∞—Ç–µ–º —Å–æ–∑–¥–∞–π—Ç–µ –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é venv, poetry, conda –∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ –≤–∞—à–µ–º—É –≤—ã–±–æ—Ä—É.</p>
                <br>

                <h4>–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π</h4>
                <p>–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (—Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å-–≤—Å—Ç–∞–≤–∏—Ç—å –≤ —Ñ–∞–π–ª requirements.txt –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å <code>pip install -r requirements.txt</code>):</p>
                <p>
                    <div class="code-wrapper">
                    <button class="copy-btn" onclick="copyCode()">Copy</button>
                    <pre><code id="code-snippet">                        numpy==1.26.4
                        torch>=2.4.0
                        tensorboard
                        torchvision
                        git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3
                        datasets==3.3.2
                        accelerate==1.4.0
                        evaluate==0.4.3
                        bitsandbytes==0.45.3
                        trl==0.15.2
                        peft==0.14.0
                        pillow==11.1.0
                        wandb==0.15.12
                        protobuf
                        sentencepiece
                    </code></pre>
                    </div>
                </p>
                <br>

                <h4>–¢–æ–∫–µ–Ω HuggingFace</h4>
                <p>–°–æ–∑–¥–∞–π—Ç–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω HuggingFace. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ–± —ç—Ç–æ–º –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å <a href="https://huggingface.co/docs/hub/en/security-tokens" target="_blank">–∑–¥–µ—Å—å</a>.</p>
                <br>

                <h4>–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞</h4>
                <p>–î–ª—è fine-tuning –í–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ JSON-—Ñ–∞–π–ª—ã —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.</p>
                <p>–ü—Ä–∏–º–µ—Ä –∫–∞—Ä—Ç–∏–Ω–∫–∏:</p>
                <img src="../images/gemma-3-fine-tuning.jpg">
                <p>–ü—Ä–∏–º–µ—Ä JSON:</p>
                <div class="code-wrapper">
<pre><code id="code-snippet">{
    "document_type": "bank_card",
    "bank": "BT",
    "num": "41************",
    "exp": "06/27",
    "cardholder": "***** *******",
    "type": "VISA"
}</code></pre>
                </div>
                <p>–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ñ–∞–π–ª—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞–∑–≤–∞–Ω—ã –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–º—É —á–∏—Å–ª–æ–≤–æ–º—É –ø–æ—Ä—è–¥–∫—É: 1.jpg, 1.json –∏ —Ç.–¥. –í —ç—Ç–æ–º —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç JPEG.</p>
                <br>

                <h4>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞</h4>
                <p>–î–∞–ª–µ–µ, –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª JSOL, –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —á–∞—Ç-—à–∞–±–ª–æ–Ω–æ–º (chat template), –∫–æ—Ç–æ—Ä—ã–π –æ–∂–∏–¥–∞–µ—Ç Gemma 3.</p>
                <p>–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª urls.txt, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π –∫–æ–¥ (–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π # —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –í–∞—à–∏–º –∫–µ–π—Å–æ–º):</p>
                <div class="code-wrapper">
                <button class="copy-btn" onclick="copyCode()">Copy</button>
                <pre><code id="code-snippet">import os
import re

from pathlib import Path


DIRECTORY = Path('.') /  # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
OUTPUT_FILE = DIRECTORY / 'urls.txt'


def natural_sort_key(file_path):
    file_name = os.path.basename(file_path)
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', file_name)]


def list_jpg_files():
    try:
        jpg_files = []
        for root, _, files in os.walk(DIRECTORY):
            for file in files:
                if file.lower().endswith(".jpg"):
                    jpg_files.append(os.path.abspath(os.path.join(root, file)))
        
        jpg_files.sort(key=natural_sort_key)
        
        with open(OUTPUT_FILE, 'w') as f:
            for file_path in jpg_files:
                f.write(file_path + "\n")
        
        print(f"File paths saved to {OUTPUT_FILE}")
    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    list_jpg_files()</code></pre>
                </div>
                <p>–° –≥–æ—Ç–æ–≤—ã–º —Ñ–∞–π–ª–æ–º <code>urls.txt</code> –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å JSONL-—Ñ–∞–π–ª. –ú–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —ç—Ç–∏–º –∫–æ–¥–æ–º:</p>
                <div class="code-wrapper">
                <button class="copy-btn" onclick="copyCode()">Copy</button>
                <pre><code id="code-snippet">import json
import os

from logging import basicConfig, INFO, getLogger
from pathlib import Path


basicConfig(level=INFO, format='%(asctime)s - %(levelname)s - %(message)s')
LOGGER = getLogger(__name__)

DATASET_PATH = Path('.') /  # Path to your dataset directory
IMAGE_URLS_FILE = DATASET_PATH / 'urls.txt'
OUTPUT_FILE = DATASET_PATH / 'dataset.jsonl'


def format_data(sample):
    return {
        "messages": [
            {"role": "system", "content": [{"type": "text", "text": sample["system_prompt"]}]},
            {"role": "user", "content": [{"type": "image", "images": sample["image"]}]},
            {"role": "assistant", "content": [{"type": "text", "text": sample["json_data"]}]},
        ],
    }


def create_jsonl(image_urls_file, data_dir, output_file):
    try:
        LOGGER.info('Reading urls file...')
        with open(image_urls_file, 'r') as f:
            image_urls = [line.strip() for line in f]
        
        LOGGER.info('Writing to output file...')
        with open(output_file, 'w') as outfile:
            for i, _ in enumerate(image_urls, start=1):
                img_json_file = os.path.join(data_dir, f'{i:d}.json')

                if os.path.exists(img_json_file):
                    with open(img_json_file, 'r') as json_file:
                        img_json = json.load(json_file)
                        formatted_json = json.dumps(img_json, indent=4, ensure_ascii=False)
                else:
                    LOGGER.warning(f"File {img_json_file} not found, skipping.")
                    continue

                image_path = os.path.join(data_dir, f'{i}.jpg')

                sample = {
                    "system_prompt": SYSTEM_PROMPT,  # Here you must provide a prompt that will guide the LLM's output.
                    "image": image_path,
                    "json_data": formatted_json,
                }

                formatted_record = format_data(sample)
                outfile.write(json.dumps(formatted_record, ensure_ascii=False) + '\n')
    except Exception:
        LOGGER.exception('Failed to create jsonl file.')


if __name__ == '__main__':
    LOGGER.info('Started creation of jsonl file...')
    create_jsonl(IMAGE_URLS_FILE, DATASET_PATH, OUTPUT_FILE)</code></pre>
                </div>
                <p>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è:
                    <li>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å.</li>
                    <li>You must provide the system prompt that corresponds to your case. You can ask ChatGPT for a good system prompt for fine-tuning in your use-case, or read <a href="#">this article</a> to know how to construct such prompts yourself.</li>
                    <li>–í–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ –í–∞—à—É –∑–∞–¥–∞—á—É. –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ—Å–∏—Ç—å –ø—Ä–æ–º–ø—Ç —É ChatGPT –∏–ª–∏ –ø—Ä–æ—á–∏—Ç–∞—Ç—å <a href="#">—ç—Ç—É —Å—Ç–∞—Ç—å—é</a>, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –∫–∞–∫ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã —Å–∞–º–æ–º—É.</li>
                    <!-- TODO: Add prompting article -->
                </p>
                <br>

                <h4>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–¥–∞</h4>
                <p>–°–æ–∑–¥–∞–π—Ç–µ <code>loader.py</code> —Ñ–∞–π–ª –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Ç—É–¥–∞ —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥ (–≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö —É–∫–∞–∑–∞–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å). –≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø–æ–¥–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:</p>
                <div class="code-wrapper">
                    <pre><code id="code-snippet">import torch

from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig


def load_model_and_processor():
    model_id = "google/gemma-3-4b-pt"  # `google/gemma-3-4b/12b/27b-pt`

    model_kwargs = dict(
        attn_implementation="eager",  # "flash_attention_2" –Ω–∞ –Ω–æ–≤—ã—Ö GPU
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    model_kwargs["quantization_config"] = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=model_kwargs["torch_dtype"],
        bnb_4bit_quant_storage=model_kwargs["torch_dtype"],
    )

    model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)
    processor = AutoProcessor.from_pretrained("google/gemma-3-4b-it")  # `google/gemma-3-4b/12b/27b-it`
    
    return model, processor</code></pre>
                </div>
                <p>–¢–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª <code>fine_tune.py</code> –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç—É–¥–∞ —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥:</p>
                <div class="code-wrapper">
                <button class="copy-btn" onclick="copyCode()">Copy</button>
                <pre><code id="code-snippet">import gc
import torch

from pathlib import Path
from PIL import Image
from logging import basicConfig, INFO, getLogger

from huggingface_hub import login
from datasets import load_dataset
from peft import LoraConfig
from trl import SFTConfig, SFTTrainer

from loader import load_model_and_processor

torch.set_float32_matmul_precision('high')
basicConfig(level=INFO, format='%(asctime)s - %(levelname)s - %(message)s')
LOGGER = getLogger(__name__)

login('')  # –í—Å—Ç–∞–≤—å—Ç–µ —Å–≤–æ–π —Ç–æ–∫–µ–Ω HuggingFace token –∑–¥–µ—Å—å
DATASET_PATH = Path('.') /  # –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º
JSONL_PATH = DATASET_PATH / 'dataset.jsonl'
MODEL_SAVE_PATH = Path('.')  # –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å

DATASET = load_dataset("json", data_files=str(JSONL_PATH), split='train')

PEFT_CONFIG = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.05,
    r=16,
    bias="none",
    target_modules="all-linear",
    task_type="CAUSAL_LM",
    modules_to_save=[
        "lm_head",
        "embed_tokens",
    ],
)

TRAINING_ARGUMENTS = SFTConfig(
    output_dir=MODEL_SAVE_PATH
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    optim="adamw_torch_fused",
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="constant",
    push_to_hub=False,
    report_to=[],
    dataset_text_field="",
    dataset_kwargs={"skip_prepare_dataset": True},
)
TRAINING_ARGUMENTS.remove_unused_columns = False

MODEL, PROCESSOR = load_model_and_processor()


def process_vision_info(messages: list[dict]) -> list[Image.Image]:
    image_inputs = []

    for message in messages:
        content = message.get("content", [])
        if not isinstance(content, list):
            content = [content]

        for element in content:
            if isinstance(element, dict) and element.get("type") == "image":
                image_path = element.get('images', None)

                if not image_path:
                    LOGGER.info('Missing image data. Skipping.')

                try:
                    image = Image.open(image_path).convert('RGB')
                    image_inputs.append(image)
                except Exception:
                    LOGGER.exception(f'Error processing image.')
                    continue

    return image_inputs


def collate_fn(examples):
    texts = []
    images = []
    for example in examples:
        image_inputs = process_vision_info(example["messages"])
        text = PROCESSOR.apply_chat_template(
            example["messages"], add_generation_prompt=False, tokenize=False
        )
        texts.append(text.strip())
        images.append(image_inputs)

    batch = PROCESSOR(text=texts, images=images, return_tensors="pt", padding=True)

    labels = batch["input_ids"].clone()

    image_token_id = [
        PROCESSOR.tokenizer.convert_tokens_to_ids(
            PROCESSOR.tokenizer.special_tokens_map["boi_token"]
        )
    ]

    labels[labels == PROCESSOR.tokenizer.pad_token_id] = -100
    labels[labels == image_token_id] = -100
    labels[labels == 262144] = -100

    batch["labels"] = labels
    return batch


if __name__ == '__main__':
    trainer = SFTTrainer(
        model=MODEL,
        args=TRAINING_ARGUMENTS,
        train_dataset=DATASET,
        peft_config=PEFT_CONFIG,
        processing_class=PROCESSOR,
        data_collator=collate_fn
    )

    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    gc.collect()

    trainer.train()
    trainer.save_model()</code></pre>
                </div>
                <p>–í —ç—Ç–æ–º —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∞ QLoRA, —Ç.–µ. –º—ã –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —Ç—Ä–µ–Ω–∏—Ä—É–µ–º –Ω–µ –º–æ–¥–µ–ª—å, –∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞. –°–ª–µ–¥—É—é—â–µ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ —Å–ª–∏—Ç—å –º–æ–¥–µ–ª—å —Å –≤–µ—Å–∞–º–∏, <strong>—ç—Ç–∞ –∑–∞–¥–∞—á–∞ —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–∞—è (—Ç—Ä–µ–±—É–µ—Ç –æ–∫–æ–ª–æ 40GB RAM).</strong></p>
                <p>–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª <code>merge.py</code> –∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Ç—É–¥–∞ —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥:</p>
                <div class="code-wrapper">
                <button class="copy-btn" onclick="copyCode()">Copy</button>
                <pre><code id="code-snippet">from pathlib import Path
from peft import PeftModel

from transformers import AutoModelForImageTextToText, AutoProcessor


MODEL_ID = "google/gemma-3-4b-pt" # `google/gemma-3-4b/12b/27b-pt`
MODEL = AutoModelForImageTextToText.from_pretrained(MODEL_ID, low_cpu_mem_usage=True)
PEFT_MODEL = PeftModel.from_pretrained(MODEL, Path('.') / "")  # –ü—É—Ç—å –∫ "–Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π" –º–æ–¥–µ–ª–∏

MERGED_MODEL = PEFT_MODEL.merge_and_unload()
MERGED_MODEL.save_pretrained("", safe_serialization=True, max_shard_size='2GB')  

PROCESSOR = AutoProcessor.from_pretrained(Path('.') / "")  # –ü—É—Ç—å –∫ "–Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π" –º–æ–¥–µ–ª–∏
PROCESSOR.save_pretrained("")  # –ü—É—Ç—å, –≥–¥–µ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å–ª–∏—Ç–∞—è –º–æ–¥–µ–ª—å</code></pre>
                </div>
                <p>–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ "" –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Å–≤–æ–∏–º –∫–µ–π—Å–æ–º</p>
                <br>

                <h4>Fine-Tuning</h4>
                <p>Now everything is ready to begin the fine-tuning process. Run the following commands from the project's root:</p>
                <p>–¢–µ–ø–µ—Ä—å –≤—Å—ë –≥–æ—Ç–æ–≤–æ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ fine-tuning. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–º–∞–Ω–¥—ã –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞:</p>
                <p></p>
                <ol>
                    <li><code>python create_image_urls.py</code></li>
                    <p>Expected output:</p>
                    <p><code>File paths saved to {output file name}</code></p>
                    <li><code>python create_jsonl.py</code></li>
                    <p>Expected output:</p>
                    <p><code>Started creation of jsonl file...</code></p>
                    <p><code>Reading urls file...</code></p>
                    <p><code>Writing to output file...</code></p>
                    <li><code>PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python fine_tune.py</code></li>
                    <p>The model should start tuning now. When it's finished, run the final command.</p>
                    <li><code>python merge.py</code></li>
                </ol>
                <br>

                <h4>Inference</h4>
                <p>We have the model. Now to run inference on it we need to setup one more script.</p>
                <p>Create <code>inference.py</code> file and paste the following code into it:</p>
                <div class="code-wrapper">
                <button class="copy-btn" onclick="copyCode()">Copy</button>
                <pre><code id="code-snippet">import torch

from logging import basicConfig, INFO, getLogger
from PIL import Image
from pathlib import Path
from transformers import AutoModelForImageTextToText, AutoProcessor


basicConfig(level=INFO, format='%(asctime)s - %(levelname)s - %(message)s')
LOGGER = getLogger(__name__)

MODEL = AutoModelForImageTextToText.from_pretrained(
    Path('.') / "",  # Provide merged model name
    device_map="auto",
    torch_dtype=torch.bfloat16,
    attn_implementation="eager",
)
PROCESSOR = AutoProcessor.from_pretrained(Path('.') / "")  # Provide merged model name


def run_inference(image_path):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},  # Use the same prompt that was used during fine-tuning
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Extract the data from this image"},
                {"type": "image", "image": Image.open(image_path)}
            ]
        },
    ]

    inputs = PROCESSOR.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = PROCESSOR(
        text=[inputs],
        images=[Image.open(image_path)],
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to(MODEL.device)

    stop_token_ids = [
        PROCESSOR.tokenizer.eos_token_id,
        PROCESSOR.tokenizer.convert_tokens_to_ids("&lt;end_of_turn&gt;")
    ]

    generated_ids = MODEL.generate(
        **inputs,
        max_new_tokens=512,
        top_p=0.9,
        do_sample=True,
        temperature=0.15,
        eos_token_id=stop_token_ids,
        disable_compile=True,
    )

    generated_ids = MODEL.generate(**inputs, max_new_tokens=256, top_p=1.0, do_sample=True, temperature=0.8, eos_token_id=stop_token_ids, disable_compile=True)
    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
    output_text = PROCESSOR.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )

    return output_text[0]


def evaluate_single_image(image_path: Path):
    json_path = image_path.with_suffix(".json")
    if not json_path.exists():
        LOGGER.error(f"JSON file not found for image: {image_path}")
        return

    start_time = time.time()
    pred = run_inference(image_path)
    elapsed = time.time() - start_time

    with open(json_path, "r", encoding="utf-8") as f:
        ground_truth = json.load(f)

    correct, total = evaluate_prediction(pred, ground_truth)

    LOGGER.info(f"{image_path.name} - Accuracy: {correct}/{total} ({correct/total if total > 0 else 0:.2%})")
    LOGGER.info(f"Inference time: {elapsed:.2f} seconds")

if __name__ == '__main__':
    sample = Path('.') / ""  # Path to an image to be passed for inference
    result = run_inference(sample, MODEL, PROCESSOR)
    LOGGER.info(result)</code></pre>
                </div>
                <p>Finally, run inference on your image:</p>
                <p><code>python inference.py path/to/file</code></p>
                <br>

                <h4>Afterword</h4>
                <p>The inference shown here is not meant to be used in production systems, as transformers library does not have the capacity for that. If you want your tuned LLM to perform in production, consider looking into <a href="#">this guide</a>.</p>
                <!--TODO: add link to vLLM article-->
            </article>
        </main>
    </div>
</body>
</html>