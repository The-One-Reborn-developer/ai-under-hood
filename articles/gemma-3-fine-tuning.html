<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" href="../styles/main.css">
    <script src="../scripts/gemma-3-fine-tuning.js" defer></script>

    <title>AI Under Hood</title>
</head>
<body>
    <div class="content">
        <header class="header-container">
            <span>
                <a href="../index.html">Main</a> Â©&nbsp;Adil Arshidin
            </span>
            <span class="header-container__nav">
                <a href="#">LLM Benchmark</a>&nbsp;Â·
                <a href="../knowledge_base.html">Knowledge Base</a>
            </span>
            <span>
                <a href="#">DE ðŸ‡©ðŸ‡ª</a>&nbsp;/
                <a href="#">RU ðŸ‡·ðŸ‡º</a>&nbsp;/
                <a href="gemma-3-fine-tuning.html">EN ðŸ‡¬ðŸ‡§</a>
            </span>
        </header>
        <main class="main-container">
            <article>
                <h3>Fine-Tuning Gemma 3 with a custom dataset for a task of data extraction from an image.</h3>
                <p><a href="https://ai.google.dev/gemma/docs/core/huggingface_vision_finetune_qlora" target="_blank">This guide</a> was used as a reference. However, the errors that occur during the process are not documented. Also, the topic of using a custom dataset is not covered. I aim to provide a more detailed guide.</p>
                <p>This guide assumes you are using an Ubuntu type Linux distribution. The only difference between using a different OS is in the prior setup to the fine-tuning.</p>
                <br>

                <h4>Environment</h4>
                <p>Python 3.11.7 will be used. You can use it locally for this project with <a href="https://github.com/pyenv/pyenv" target="_blank">pyenv</a>. With pyenv installed and configured, navigate to the project's directory and execute: <code>pyenv local 3.11.7</code>.</p>
                <p>Now create and activate the virtual environment using venv, poetry, conda or whatever else you prefer.</p>
                <br>

                <h4>Dependencies</h4>
                <p>Install the following dependencies (you can also copy and paste the following block into requirements.txt file in your project's root and run <code>pip install -r requirements.txt</code>):</p>
                <p>
                    <div class="code-wrapper">
                    <button class="copy-btn" onclick="copyCode()">Copy</button>
                    <pre><code id="code-snippet">                        numpy==1.26.4
                        torch>=2.4.0
                        tensorboard
                        torchvision
                        git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3
                        datasets==3.3.2
                        accelerate==1.4.0
                        evaluate==0.4.3
                        bitsandbytes==0.45.3
                        trl==0.15.2
                        peft==0.14.0
                        pillow==11.1.0
                        wandb==0.15.12
                        protobuf
                        sentencepiece
                    </code></pre>
                    </div>
                </p>
                <br>

                <h4>HuggingFace token</h4>
                <p>You will need to create or take your existing HuggingFace token. You can read more on that <a href="https://huggingface.co/docs/hub/en/security-tokens" target="_blank">here</a>.</p>
                <br>

                <h4>Create the dataset</h4>
                <p>For fine-tuning you'll need labeled images, that is, the images the data needs to be extracted from and the corresponding json file with the extracted data already labeled.</p>
                <p>Example image:</p>
                <img src="../images/gemma-3-fine-tuning.jpg">
                <p>Example JSON:</p>
                <div class="code-wrapper">
<pre><code id="code-snippet">{
    "document_type": "bank_card",
    "bank": "BT",
    "num": "41************",
    "exp": "06/27",
    "cartholder": "***** *******",
    "type": "VISA"
}</code></pre>
                </div>
                <p>For ease of use the files should be named numerically ascending: 1.jpg, 1.json, etc. Note that I will use JPEG format for this guide.</p>
                <br>

                <h4>Prepare the dataset</h4>
                <p>Next you must create a jsonl file in the format of the chat template that Gemma 3 will receive as input.</p>
                <p>First, create the file with image paths (or urls). You can use this code (The # comment specifies what you should change according to your setup):</p>
                <div class="code-wrapper">
                <button class="copy-btn" onclick="copyCode()">Copy</button>
                <pre><code id="code-snippet">import os
import re

from pathlib import Path


DIRECTORY = Path('.') /  # Provide the path to your dataset directory here
OUTPUT_FILE = DIRECTORY / 'urls.txt'


def natural_sort_key(file_path):
    file_name = os.path.basename(file_path)
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', file_name)]


def list_jpg_files():
    try:
        jpg_files = []
        for root, _, files in os.walk(DIRECTORY):
            for file in files:
                if file.lower().endswith(".jpg"):
                    jpg_files.append(os.path.abspath(os.path.join(root, file)))
        
        jpg_files.sort(key=natural_sort_key)
        
        with open(OUTPUT_FILE, 'w') as f:
            for file_path in jpg_files:
                f.write(file_path + "\n")
        
        print(f"File paths saved to {OUTPUT_FILE}")
    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    list_jpg_files()</code></pre>
                </div>
                <p>Secondly, with the urls.txt file successfully created and in your dataset directory, you can now create the jsonl file. Again, you can use this code:</p>
                <div class="code-wrapper">
                <button class="copy-btn" onclick="copyCode()">Copy</button>
                <pre><code id="code-snippet">import json
import os

from logging import basicConfig, INFO, getLogger
from pathlib import Path


basicConfig(level=INFO, format='%(asctime)s - %(levelname)s - %(message)s')
LOGGER = getLogger(__name__)

DATASET_PATH = Path('.') /  # Path to your dataset directory
IMAGE_URLS_FILE = DATASET_PATH / 'urls.txt'
OUTPUT_FILE = DATASET_PATH / 'dataset.jsonl'


def format_data(sample):
    return {
        "messages": [
            {"role": "system", "content": [{"type": "text", "text": sample["system_prompt"]}]},
            {"role": "user", "content": [{"type": "image", "images": sample["image"]}]},
            {"role": "assistant", "content": [{"type": "text", "text": sample["json_data"]}]},
        ],
    }


def create_jsonl(image_urls_file, data_dir, output_file):
    try:
        LOGGER.info('Reading urls file...')
        with open(image_urls_file, 'r') as f:
            image_urls = [line.strip() for line in f]
        
        LOGGER.info('Writing to output file...')
        with open(output_file, 'w') as outfile:
            for i, _ in enumerate(image_urls, start=1):
                img_json_file = os.path.join(data_dir, f'{i:d}.json')

                if os.path.exists(img_json_file):
                    with open(img_json_file, 'r') as json_file:
                        img_json = json.load(json_file)
                        formatted_json = json.dumps(img_json, indent=4, ensure_ascii=False)
                else:
                    LOGGER.warning(f"File {img_json_file} not found, skipping.")
                    continue

                image_path = os.path.join(data_dir, f'{i}.jpg')

                sample = {
                    "system_prompt": SYSTEM_PROMPT,  # Here you must provide a prompt that will guide the LLM's output.
                    "image": image_path,
                    "json_data": formatted_json,
                }

                formatted_record = format_data(sample)
                outfile.write(json.dumps(formatted_record, ensure_ascii=False) + '\n')
    except Exception:
        LOGGER.exception('Failed to create jsonl file.')


if __name__ == '__main__':
    LOGGER.info('Started creation of jsonl file...')
    create_jsonl(IMAGE_URLS_FILE, DATASET_PATH, OUTPUT_FILE)</code></pre>
                </div>
                <p>Couple of things to note so far:
                    <li>You must provide your own path to dataset directory.</li>
                    <li>You must provide the system prompt that corresponds to your case. You can ask ChatGPT for a good system prompt for fine-tuning in your use-case, or read <a href="#">this article</a> to know how to construct such prompts yourself.</li>
                    <!-- TODO: Add prompting article -->
                </p>
                <br>

                <h4>Fine-Tuning</h4>
                <p>Now everything is set to do the job. Create a loader.py file in your project's root and paste the following code in it (The comments also list the various gemma models you can choose from). This is a helper module that will load the model and its processor:</p>
                <div class="code-wrapper">
                    <pre><code id="code-snippet">import torch

from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig


def load_model_and_processor():
    model_id = "google/gemma-3-4b-pt"  # `google/gemma-3-4b/12b/27b-pt`

    model_kwargs = dict(
        attn_implementation="eager",  # "flash_attention_2" on newer GPUs
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    model_kwargs["quantization_config"] = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=model_kwargs["torch_dtype"],
        bnb_4bit_quant_storage=model_kwargs["torch_dtype"],
    )

    model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)
    processor = AutoProcessor.from_pretrained("google/gemma-3-4b-it")  # `google/gemma-3-4b/12b/27b-it`
    
    return model, processor</code></pre>
                </div>
                <p></p>
            </article>
        </main>
    </div>
</body>
</html>